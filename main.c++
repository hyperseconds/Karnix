#include <iostream>
#include <vector>
#include <cmath>
#include <iomanip>
#include <string>
#include <algorithm>
#include <random>
#include <cassert>
#include <numeric>
#include <functional>
#include <cstdlib>
#include <ctime>

#include "mathematical/Vector/VectorOperations.hpp"
#include "mathematical/matrix/MatrixOperations.hpp"
#include "mathematical/matrix/AdvancedMatrix.hpp"
#include "mathematical/Calc/Calc.hpp"
#include "mathematical/Calc/CalcEng.hpp"
#include "mathematical/Graph/GraphVisualizer.hpp"
#include "mathematical/ComputationalGraph/ComputationalGraph.hpp"

#include "neural_network/utils/Tensor.hpp"
#include "neural_network/utils/ImageProcessor.hpp"
#include "neural_network/cnn/CNNArchitecture.hpp"
#include "neural_network/layers/ActivationFunctions.hpp"
#include "neural_network/layers/ConvolutionLayer.hpp"
#include "neural_network/layers/PoolingLayers.hpp"
#include "neural_network/layers/FullyConnectedLayer.hpp"
#include "neural_network/optimizers/Optimizers.hpp"
#include "neural_network/utils/LossFunctions.hpp"
#include "neural_network/tests/GradientChecker.hpp"

#include "medical/MedicalImageProcessor.hpp"
#include "medical/MedicalCNN.hpp"
#include "medical/MedicalEvaluationUtils.hpp"

using namespace std;

int main(){
    srand(static_cast<unsigned>(time(nullptr)));
    
    cout << string(80, '=') << endl;
    cout << "KARNIX - COMPLETE NEURAL NETWORK & MATHEMATICAL COMPUTING SYSTEM" << endl;
    cout << string(80, '=') << endl;
    
    cout << "\n=== MATHEMATICAL FOUNDATIONS ===" << endl;
    
    cout << "\n--- Vector Operations ---" << endl;
    VectorOperations vo;
    int arr1[] = {3, 4, 5};
    int arr2[] = {1, 2, 3};
    vo.setVectors(arr1, 3, arr2, 3);
    
    cout << "Vector 1: ";
    vo.displayVector1();
    cout << "Vector 2: ";
    vo.displayVector2();
    
    vo.vector_add();
    cout << "Addition Result: ";
    vo.displayAddResult();
    
    vo.vector_sub();
    cout << "Subtraction Result: ";
    vo.displaySubResult();
    
    vo.scalarVectorMultiply(2);
    cout << "Scalar Multiplication Result: ";
    vo.displayScalarResult();
    
    vo.vector_norm();
    cout << "Vector Norm (Magnitude): ";
    vo.displayNormResult();

    cout << "\n--- Matrix Operations ---" << endl;
    MatrixOperations mo;
    
    vector<vector<int>> matrix1 = {
        {1, 2, 3},
        {4, 5, 6}
    };
    
    vector<vector<int>> matrix2 = {
        {7, 8, 9},
        {10, 11, 12}
    };
    
    mo.setMatrices(matrix1, matrix2);
    mo.displayMatrix1();
    mo.displayMatrix2();
    
    mo.matrix_add();
    mo.displayAddResult();
    
    mo.matrix_sub();
    mo.displaySubResult();
    
    mo.setMatrix(matrix1);
    mo.scalar_multiply(3);
    mo.displayScalarResult();
    
    mo.matrix_transpose();
    mo.displayTransposeResult();
    
    mo.displayNorm();

    cout << "\n=== Advanced Matrix Operations ===" << endl;
    AdvancedMatrix am;
    
    // Create a 2x2 matrix for eigenvalue demonstration
    vector<vector<double>> square_matrix = {
        {4.0, 2.0},
        {1.0, 3.0}
    };
    
    am.setMatrix(square_matrix);
    am.displayMatrix();
    
    // Matrix trace
    am.displayTrace();
    
    // Matrix rank
    am.displayRank();
    
    // Eigenvalues (conceptual)
    am.displayEigenvalues();
    
    // Eigenvectors (conceptual)
    am.displayEigenvectors();
    
    // Matrix projection
    vector<double> vector_b = {5.0, 3.0};
    am.displayProjection(vector_b);
    
    // Test with a 3x3 matrix
    cout << "\n=== 3x3 Matrix Example ===" << endl;
    vector<vector<double>> matrix_3x3 = {
        {1.0, 2.0, 3.0},
        {4.0, 5.0, 6.0},
        {7.0, 8.0, 9.0}
    };
    
    am.setMatrix(matrix_3x3);
    am.displayMatrix();
    am.displayTrace();
    am.displayRank();
    am.displayEigenvalues(); // Will show conceptual message
    
    vector<double> vector_b2 = {1.0, 2.0, 3.0};
    am.displayProjection(vector_b2);

    cout << "\n=== Calculus Operations ===" << endl;
    Calc calc;
    double x = 2.0;
    double y = 3.0;
    
    // Constant function
    cout << "Constant function f(x) = 5: " << calc.constant(x, 5.0) << endl;
    cout << "Derivative of constant: " << calc.constant_derivative(x) << endl;
    
    // Linear function
    cout << "Linear function f(x) = 2x + 3: " << calc.linear(x, 2.0, 3.0) << endl;
    
    // Polynomial function
    vector<double> coeffs = {1.0, 2.0, 3.0}; // 1 + 2x + 3xÂ²
    cout << "Polynomial f(x) = 1 + 2x + 3xÂ² at x=" << x << ": " << calc.polynomial(x, coeffs) << endl;
    cout << "Polynomial derivative at x=" << x << ": " << calc.polynomial_derivative(x, coeffs) << endl;
    
    // Exponential function
    cout << "Exponential e^x at x=" << x << ": " << calc.exponential(x) << endl;
    cout << "Exponential derivative at x=" << x << ": " << calc.exponential_derivative(x) << endl;
    
    // Logarithmic function
    cout << "Natural log ln(x) at x=" << x << ": " << calc.logarithmic(x) << endl;
    cout << "Logarithmic derivative at x=" << x << ": " << calc.logarithmic_derivative(x) << endl;
    
    // Trigonometric functions
    cout << "Sine sin(x) at x=" << x << ": " << calc.sine(x) << endl;
    cout << "Sine derivative cos(x) at x=" << x << ": " << calc.sine_derivative(x) << endl;
    cout << "Cosine cos(x) at x=" << x << ": " << calc.cosine(x) << endl;
    cout << "Cosine derivative -sin(x) at x=" << x << ": " << calc.cosine_derivative(x) << endl;
    
    // Composite function
    cout << "Composite function f(g(x)) where g(x)=x+1, f(u)=uÂ² at x=" << x << ": " << calc.f_of_g(x) << endl;
    cout << "Composite derivative using chain rule at x=" << x << ": " << calc.composite_derivative(x) << endl;
    
    // Multivariate function
    cout << "Multivariate f(x,y) = xÂ² + yÂ² at (" << x << "," << y << "): " << calc.multivariate(x, y) << endl;
    auto gradient = calc.multivariate_gradient(x, y);
    cout << "Gradient at (" << x << "," << y << "): (" << gradient.first << ", " << gradient.second << ")" << endl;

    cout << "\n=== AI/ML Advanced Calculus ===" << endl;
    CalcEng calcEng;
    
    // Demonstrate the 5 AI calculus concepts
    calcEng.demonstrateAICalculus();
    
    // Individual demonstrations
    cout << "\n--- Individual AI Calculus Operations ---" << endl;
    
    // 1. Differentiation for neural network backpropagation
    auto relu = [&calcEng](double x) { return calcEng.neural_activation_relu(x); };
    calcEng.differentiation(relu, 2.0);
    calcEng.displayDifferentiation(2.0);
    
    // 2. Integration for probability calculations
    auto probability_density = [](double x) { return std::exp(-x*x/2) / std::sqrt(2*M_PI); };
    calcEng.integration(probability_density, -1.0, 1.0);
    calcEng.displayIntegration(-1.0, 1.0);
    
    // 3. Gradient for loss function optimization
    auto loss_func = [](const std::vector<double>& w) {
        return w[0]*w[0] + w[1]*w[1] + 2*w[0]*w[1]; // Simple quadratic loss
    };
    std::vector<double> weights = {1.5, 2.5};
    calcEng.gradient(loss_func, weights);
    calcEng.displayGradient(weights);
    
    // 4. Jacobian for neural network layer transformations
    std::vector<std::function<double(std::vector<double>)>> network_funcs = {
        [](const std::vector<double>& x) { return x[0] + x[1]; },
        [](const std::vector<double>& x) { return x[0] * x[1]; }
    };
    std::vector<double> inputs = {1.0, 2.0};
    calcEng.jacobian(network_funcs, inputs);
    calcEng.displayJacobian(inputs);
    
    // 5. Hessian for second-order optimization (Newton's method)
    auto optimization_func = [](const std::vector<double>& x) {
        return x[0]*x[0] + x[1]*x[1] - 2*x[0] - 4*x[1] + 5; // Convex quadratic
    };
    std::vector<double> opt_point = {0.5, 1.0};
    calcEng.hessian(optimization_func, opt_point);
    calcEng.displayHessian(opt_point);

    cout << "\n=== Graph Visualizations ===" << endl;
    GraphVisualizer graph;
    
    // 1. Sine Wave
    cout << "\n1. Trigonometric Function:" << endl;
    graph.plotSineWave();
    
    // 2. Quadratic Function
    cout << "\n2. Quadratic Function:" << endl;
    graph.plotQuadratic();
    
    // 3. Exponential Function
    cout << "\n3. Exponential Growth:" << endl;
    graph.plotExponential();
    
    // 4. Logarithmic Function
    cout << "\n4. Logarithmic Function:" << endl;
    graph.plotLogarithmic();
    
    // 5. Function and its Derivative
    cout << "\n5. Function and Derivative Comparison:" << endl;
    graph.plotDerivative();
    
    // 6. Gradient Field Visualization
    cout << "\n6. Gradient Field (AI Optimization Context):" << endl;
    graph.plotGradientField();
    
    // 7. Contour Plot
    cout << "\n7. Contour Plot (Loss Function Landscape):" << endl;
    graph.plotContour();
    
    // 8. Data Scatter Plot with Trend Line
    cout << "\n8. Data Analysis - Scatter Plot with Regression:" << endl;
    graph.plotDataScatter();
    
    // 9. Custom AI/ML Function Visualization
    cout << "\n9. Custom AI Function Visualization:" << endl;
    graph.setTitle("Neural Network Activation: Sigmoid vs ReLU");
    graph.setRange(-5, 5, -0.5, 5);
    graph.clearCanvas();
    graph.drawAxes();
    
    // Plot sigmoid activation function
    graph.plotFunction([](double x) { return 1.0 / (1.0 + std::exp(-x)); }, 's');
    
    // Plot ReLU activation function
    graph.plotFunction([](double x) { return std::max(0.0, x); }, 'r');
    
    graph.addScale();
    graph.display();
    cout << "Legend: s = Sigmoid, r = ReLU" << endl;
    
    // 10. Loss Function Visualization
    cout << "\n10. Loss Function Landscape:" << endl;
    graph.setTitle("Quadratic Loss Function: L = (x-2)Â² + (y-1)Â²");
    graph.setRange(-1, 5, -2, 4);
    graph.clearCanvas();
    graph.drawAxes();
    
    // Plot minimum point
    std::vector<std::pair<double, double>> minimum = {{2.0, 1.0}};
    graph.plotPoints(minimum, 'M');
    
    // Plot some gradient descent path
    std::vector<std::pair<double, double>> descent_path = {
        {4.5, 3.5}, {4.0, 3.0}, {3.5, 2.5}, {3.0, 2.0}, {2.5, 1.5}, {2.0, 1.0}
    };
    graph.plotPoints(descent_path, '*');
    
    graph.addScale();
    graph.display();
    cout << "M = Global Minimum, * = Gradient Descent Path" << endl;

    cout << "\n=== Computational Graph & Autograd ===" << endl;
    
    // 1. Simple operations demonstration
    AutogradDemo::simple_operations_demo();
    
    // 2. Neural network backpropagation
    AutogradDemo::neural_network_demo();
    
    // 3. Chain rule demonstration
    AutogradDemo::chain_rule_demo();
    
    // 4. Advanced autograd example - polynomial regression
    cout << "\n=== Polynomial Regression with Autograd ===" << endl;
    comp_graph.clear();
    
    // Data point: y = 2x^3 - 3x^2 + x + 1 at x = 2
    auto x_data = Constant(2.0, "x_data");
    auto y_target = Constant(2*8 - 3*4 + 2 + 1, "y_target"); // = 11
    
    // Model parameters (to be learned)
    auto a = Variable(1.0, "a");  // coefficient for x^3
    auto b = Variable(1.0, "b");  // coefficient for x^2  
    auto c = Variable(1.0, "c");  // coefficient for x
    auto d = Variable(1.0, "d");  // constant term
    
    // Model: y = a*x^3 + b*x^2 + c*x + d
    auto x_cubed = Power(x_data, 3.0, "x^3");
    auto x_squared = Power(x_data, 2.0, "x^2");
    
    auto term1 = Multiply(a, x_cubed, "a*x^3");
    auto term2 = Multiply(b, x_squared, "b*x^2");
    auto term3 = Multiply(c, x_data, "c*x");
    
    auto sum1 = Add(term1, term2, "a*x^3+b*x^2");
    auto sum2 = Add(sum1, term3, "...+c*x");
    auto y_pred = Add(sum2, d, "y_predicted");
    
    // Loss function
    auto loss = MSELoss(y_pred, y_target, "polynomial_loss");
    
    cout << "Polynomial Model: y = a*x^3 + b*x^2 + c*x + d" << endl;
    cout << "Data point: x = " << x_data->scalar_value() << ", y_target = " << y_target->scalar_value() << endl;
    cout << "Initial prediction: " << y_pred->scalar_value() << endl;
    cout << "Initial loss: " << loss->scalar_value() << endl;
    
    // Backward pass
    comp_graph.backward(loss);
    
    cout << "\nGradients for parameter updates:" << endl;
    cout << "âˆ‚L/âˆ‚a = " << a->scalar_grad() << " (should move a towards 2)" << endl;
    cout << "âˆ‚L/âˆ‚b = " << b->scalar_grad() << " (should move b towards -3)" << endl;
    cout << "âˆ‚L/âˆ‚c = " << c->scalar_grad() << " (should move c towards 1)" << endl;
    cout << "âˆ‚L/âˆ‚d = " << d->scalar_grad() << " (should move d towards 1)" << endl;
    
    // 5. Activation function comparison
    cout << "\n=== Activation Function Gradients ===" << endl;
    comp_graph.clear();
    
    auto x_act = Variable(0.5, "x");
    auto sigmoid_out = Sigmoid(x_act, "sigmoid");
    auto relu_out = ReLU(x_act, "relu");
    
    cout << "Input: " << x_act->scalar_value() << endl;
    cout << "Sigmoid output: " << sigmoid_out->scalar_value() << endl;
    cout << "ReLU output: " << relu_out->scalar_value() << endl;
    
    // Test sigmoid gradient
    comp_graph.backward(sigmoid_out);
    double sigmoid_grad = x_act->scalar_grad();
    
    // Reset gradients
    x_act->grad[0] = 0.0;
    
    // Test ReLU gradient  
    comp_graph.backward(relu_out);
    double relu_grad = x_act->scalar_grad();
    
    cout << "Sigmoid gradient: " << sigmoid_grad << endl;
    cout << "ReLU gradient: " << relu_grad << endl;

    cout << "\n" << string(60, '=') << endl;
    cout << "DEEP LEARNING NEURAL NETWORK COMPONENTS" << endl;
    cout << string(60, '=') << endl;

    // ===== TENSOR OPERATIONS =====
    cout << "\n=== Tensor Operations ===" << endl;
    
    // Create tensors
    Tensor input_tensor({2, 3, 4}, true); // [C, H, W] format
    input_tensor.random_normal(0.0, 1.0);
    input_tensor.print("Input Tensor");
    
    Tensor weight_tensor({5, 12}, false); // Fully connected weights
    weight_tensor.xavier_init();
    cout << "Weight tensor initialized with Xavier initialization" << endl;
    
    // ===== ACTIVATION FUNCTIONS =====
    cout << "\n=== Activation Functions ===" << endl;
    
    // Test data for activations
    Tensor activation_input({1, 5}, false);
    activation_input.get_data() = {-2.0, -1.0, 0.0, 1.0, 2.0};
    activation_input.print("Activation Input");
    
    // ReLU
    ActivationFunctions::ReLU relu_activation;
    Tensor relu_output = relu_activation.forward(activation_input);
    relu_output.print("ReLU Output");
    
    Tensor relu_grad_out({1, 5}, false);
    relu_grad_out.fill(1.0);
    Tensor relu_grad_in = relu_activation.backward(relu_grad_out);
    relu_grad_in.print("ReLU Gradient Input");
    
    // Sigmoid
    ActivationFunctions::Sigmoid sigmoid;
    Tensor sigmoid_output = sigmoid.forward(activation_input);
    sigmoid_output.print("Sigmoid Output");
    
    // Softmax
    ActivationFunctions::Softmax softmax_func;
    Tensor softmax_logits({1, 4}, false);
    softmax_logits.get_data() = {1.0, 2.0, 3.0, 4.0};
    Tensor softmax_output = softmax_func.forward(softmax_logits);
    softmax_output.print("Softmax Output");
    
    // ===== CONVOLUTION LAYER =====
    cout << "\n=== Convolution Layer ===" << endl;
    
    ConvolutionLayer conv_layer(3, 16, 3, 1, 1); // 3->16 channels, 3x3 kernel, stride=1, padding=1
    conv_layer.print_info();
    
    Tensor conv_input({3, 8, 8}, false); // [C, H, W]
    conv_input.random_normal(0.0, 0.5);
    cout << "Convolution input shape: [3, 8, 8]" << endl;
    
    Tensor conv_output = conv_layer.forward(conv_input);
    conv_output.print("Convolution Output");
    
    // Backward pass
    Tensor conv_grad_output(conv_output.get_shape(), false);
    conv_grad_output.fill(0.1);
    Tensor conv_grad_input = conv_layer.backward(conv_grad_output);
    cout << "Convolution backward pass completed" << endl;
    cout << "Weight gradient norm: " << conv_layer.get_weight_grad().sum() << endl;
    
    // ===== POOLING LAYERS =====
    cout << "\n=== Pooling Layers ===" << endl;
    
    // Max Pooling
    MaxPoolingLayer max_pool(2, 2); // 2x2 pooling, stride=2
    max_pool.print_info();
    
    Tensor pool_input({2, 4, 4}, false);
    pool_input.random_normal(0.0, 1.0);
    pool_input.print("Pooling Input");
    
    Tensor max_pool_output = max_pool.forward(pool_input);
    max_pool_output.print("Max Pool Output");
    
    // Average Pooling
    AveragePoolingLayer avg_pool(2, 2);
    Tensor avg_pool_output = avg_pool.forward(pool_input);
    avg_pool_output.print("Average Pool Output");
    
    // Global Average Pooling
    GlobalAveragePoolingLayer global_avg_pool;
    Tensor global_pool_output = global_avg_pool.forward(pool_input);
    global_pool_output.print("Global Average Pool Output");
    
    // ===== FULLY CONNECTED LAYER =====
    cout << "\n=== Fully Connected Layer ===" << endl;
    
    FullyConnectedLayer fc_layer(8, 3, true); // 8 inputs, 3 outputs, use bias
    fc_layer.print_info();
    
    Tensor fc_input({8}, false);
    fc_input.random_normal(0.0, 0.5);
    fc_input.print("FC Input");
    
    Tensor fc_output = fc_layer.forward(fc_input);
    fc_output.print("FC Output");
    
    // Backward pass
    Tensor fc_grad_output({3}, false);
    fc_grad_output.get_data() = {0.1, -0.2, 0.3};
    Tensor fc_grad_input = fc_layer.backward(fc_grad_output);
    fc_grad_input.print("FC Gradient Input");
    
    // ===== OPTIMIZERS =====
    cout << "\n=== Optimizers ===" << endl;
    
    // Create test parameters and gradients
    Tensor parameters({2, 2}, false);
    parameters.get_data() = {1.0, 2.0, 3.0, 4.0};
    parameters.print("Initial Parameters");
    
    Tensor gradients({2, 2}, false);
    gradients.get_data() = {0.1, 0.2, 0.3, 0.4};
    gradients.print("Gradients");
    
    // SGD
    SGD sgd_optimizer(0.1);
    sgd_optimizer.print_info();
    sgd_optimizer.update(parameters, gradients);
    parameters.print("Parameters after SGD update");
    
    // Reset parameters
    parameters.get_data() = {1.0, 2.0, 3.0, 4.0};
    
    // Adam
    Adam adam_optimizer(0.01);
    adam_optimizer.print_info();
    for (int i = 0; i < 3; ++i) {
        adam_optimizer.update(parameters, gradients);
        cout << "Adam step " << (i+1) << ": param[0] = " << parameters[0] << endl;
    }
    
    // ===== LOSS FUNCTIONS =====
    cout << "\n=== Loss Functions ===" << endl;
    
    // MSE Loss
    Tensor predictions({3}, false);
    predictions.get_data() = {1.0, 2.0, 3.0};
    Tensor targets({3}, false);
    targets.get_data() = {1.1, 1.9, 3.2};
    
    double mse_loss = MSELoss::forward(predictions, targets);
    cout << "MSE Loss: " << mse_loss << endl;
    
    Tensor mse_grad = MSELoss::backward(predictions, targets);
    mse_grad.print("MSE Gradient");
    
    // Cross-Entropy Loss
    Tensor ce_logits({3}, false);
    ce_logits.get_data() = {1.0, 2.0, 0.5};
    Tensor ce_targets({3}, false);
    ce_targets.get_data() = {0.0, 1.0, 0.0}; // One-hot encoded
    
    double ce_loss = CrossEntropyLoss::forward(ce_logits, ce_targets);
    cout << "Cross-Entropy Loss: " << ce_loss << endl;
    
    Tensor ce_grad = CrossEntropyLoss::backward(ce_logits, ce_targets);
    ce_grad.print("Cross-Entropy Gradient");
    
    // ===== SIMPLE NEURAL NETWORK =====
    cout << "\n=== Simple Neural Network Example ===" << endl;
    
    // Create a simple 2-layer network: Input(4) -> Hidden(8) -> Output(2)
    FullyConnectedLayer layer1(4, 8, true);
    FullyConnectedLayer layer2(8, 2, true);
    ActivationFunctions::ReLU hidden_activation;
    ActivationFunctions::Softmax output_activation;
    
    // Training data
    Tensor nn_input({4}, false);
    nn_input.get_data() = {0.5, -0.2, 0.8, -0.1};
    nn_input.print("Network Input");
    
    Tensor nn_target({2}, false);
    nn_target.get_data() = {1.0, 0.0}; // Class 0
    nn_target.print("Target");
    
    // Forward pass
    Tensor hidden = layer1.forward(nn_input);
    Tensor hidden_activated = hidden_activation.forward(hidden);
    Tensor output_logits = layer2.forward(hidden_activated);
    Tensor output_probs = output_activation.forward(output_logits);
    
    output_probs.print("Network Output Probabilities");
    
    // Compute loss
    double network_loss = CrossEntropyLoss::forward(output_logits, nn_target);
    cout << "Network Loss: " << network_loss << endl;
    
    // Backward pass
    Tensor loss_grad = CrossEntropyLoss::backward(output_logits, nn_target);
    Tensor output_grad = layer2.backward(loss_grad);
    Tensor hidden_grad = hidden_activation.backward(output_grad);
    Tensor input_grad = layer1.backward(hidden_grad);
    
    cout << "Backward pass completed!" << endl;
    cout << "Layer 1 weight gradient sum: " << layer1.get_weight_grad().sum() << endl;
    cout << "Layer 2 weight gradient sum: " << layer2.get_weight_grad().sum() << endl;
    
    // ===== GRADIENT CHECKING =====
    cout << "\n=== Gradient Checking ===" << endl;
    
    GradientChecker checker;
    
    // Test MSE loss gradients
    Tensor test_pred({3}, false);
    test_pred.get_data() = {1.0, 2.0, 3.0};
    Tensor test_target({3}, false);
    test_target.get_data() = {1.5, 1.8, 3.2};
    
    bool mse_check = checker.check_loss_gradients(test_pred, test_target, "MSE");
    cout << "MSE gradient check: " << (mse_check ? "PASSED" : "FAILED") << endl;
    
    // ===== TRAINING DEMONSTRATION =====
    cout << "\n=== Mini Training Loop ===" << endl;
    
    // Simple training loop for the 2-layer network
    Adam network_optimizer(0.01);
    
    for (int epoch = 0; epoch < 5; ++epoch) {
        // Forward pass
        Tensor h1 = layer1.forward(nn_input);
        Tensor h1_act = hidden_activation.forward(h1);
        Tensor h2 = layer2.forward(h1_act);
        Tensor output = output_activation.forward(h2);
        
        // Loss
        double loss = CrossEntropyLoss::forward(h2, nn_target);
        
        // Backward pass
        Tensor grad_loss = CrossEntropyLoss::backward(h2, nn_target);
        Tensor grad_h2 = layer2.backward(grad_loss);
        Tensor grad_h1_act = hidden_activation.backward(grad_h2);
        layer1.backward(grad_h1_act);
        
        // Update parameters
        network_optimizer.update(layer1.get_weights(), layer1.get_weight_grad());
        network_optimizer.update(layer1.get_bias(), layer1.get_bias_grad());
        network_optimizer.update(layer2.get_weights(), layer2.get_weight_grad());
        network_optimizer.update(layer2.get_bias(), layer2.get_bias_grad());
        
        cout << "Epoch " << epoch + 1 << " - Loss: " << loss 
             << " - Prediction: [" << output[0] << ", " << output[1] << "]" << endl;
    }
    
    cout << "\n=== CNN Architecture Example ===" << endl;
    
    ImageProcessor processor(true);
    
    vector<string> image_patterns = {"gradient", "checkerboard", "circles", "noise", "edges"};
    vector<Tensor> test_images;
    vector<string> image_names;
    
    cout << "Creating synthetic test images for CNN..." << endl;
    for (int i = 0; i < image_patterns.size(); ++i) {
        ImageProcessor::Image img = processor.create_test_image(32, 32, image_patterns[i]);
        Tensor img_tensor = processor.image_to_tensor(img, true, false); // RGB
        test_images.push_back(img_tensor);
        image_names.push_back(image_patterns[i]);
        cout << "Created " << image_patterns[i] << " pattern (32x32 RGB)" << endl;
    }
    
    cout << "\nVisualizing checkerboard pattern:" << endl;
    processor.visualize_tensor_ascii(test_images[1], 0, 32);
    
    CNNArchitecture cnn(3, 32, 32, 5, 0.001, true);
    
    cout << "\nProcessing images through CNN..." << endl;
    cout << string(70, '-') << endl;
    cout << setw(15) << "Pattern" << setw(15) << "Predicted" << setw(15) << "Confidence" 
         << setw(25) << "Top Classes" << endl;
    cout << string(70, '-') << endl;
    
    for (int i = 0; i < test_images.size(); ++i) {
        cnn.set_verbose(false);
        vector<double> scores = cnn.get_confidence_scores(test_images[i]);
        int pred = cnn.predict(test_images[i]);
        
        vector<pair<double, int>> score_pairs;
        for (int j = 0; j < scores.size(); ++j) {
            score_pairs.push_back({scores[j], j});
        }
        sort(score_pairs.rbegin(), score_pairs.rend());
        
        cout << setw(15) << image_patterns[i] 
             << setw(15) << ("Class_" + to_string(pred))
             << setw(15) << fixed << setprecision(3) << scores[pred]
             << setw(12) << ("C" + to_string(score_pairs[0].second)) << "(" << setprecision(2) << score_pairs[0].first << ")"
             << setw(12) << ("C" + to_string(score_pairs[1].second)) << "(" << setprecision(2) << score_pairs[1].first << ")"
             << endl;
    }
    
    cout << "\nDetailed CNN analysis for circles pattern:" << endl;
    cnn.set_verbose(true);
    cnn.forward(test_images[2]);
    cnn.visualize_feature_maps();
    cnn.create_activation_heatmap();
    
    cout << "\nCNN Training Simulation:" << endl;
    std::pair<std::vector<Tensor>, std::vector<Tensor>> dataset = CNNUtils::create_synthetic_dataset(10, 32, 32);
    std::vector<Tensor> train_images = dataset.first;
    std::vector<Tensor> train_labels = dataset.second;
    cout << "Created training dataset with " << train_images.size() << " samples" << endl;
    
    cnn.set_verbose(false);
    CNNUtils::train_cnn_epoch(cnn, train_images, train_labels, 1);
    
    cout << "\n=== Advanced Feature Visualization ===" << endl;
    
    cout << "Visualizing learned feature patterns:" << endl;
    cnn.set_verbose(true);
    
    for (int i = 0; i < min(3, (int)test_images.size()); ++i) {
        cout << "\nAnalyzing " << image_patterns[i] << " pattern:" << endl;
        cout << string(50, '-') << endl;
        
        cnn.forward(test_images[i]);
        vector<double> scores = cnn.get_confidence_scores(test_images[i]);
        
        cout << "Feature extraction summary:" << endl;
        cout << "- Conv1 features: Edge detection patterns" << endl;
        cout << "- Conv2 features: Texture combinations" << endl;
        cout << "- Conv3 features: High-level pattern recognition" << endl;
        
        cout << "Classification probabilities:" << endl;
        for (int j = 0; j < scores.size(); ++j) {
            cout << "  Class " << j << ": " << fixed << setprecision(4) << scores[j] << endl;
        }
    }
    
    cout << "\n" << string(80, '=') << endl;
    cout << "ðŸ¥ MEDICAL AI: BRAIN TUMOR DETECTION SYSTEM" << endl;
    cout << string(80, '=') << endl;
    
    cout << "\n=== Medical CNN Architecture for Brain Tumor Detection ===" << endl;
    
    MedicalCNN medical_cnn(1, 64, 64, true);
    
    cout << "\n=== Creating Synthetic Clinical Dataset ===" << endl;
    MedicalEvaluationUtils::ClinicalDataset clinical_data = 
        MedicalEvaluationUtils::create_clinical_dataset(16, 64);
    
    cout << "\n=== Individual Case Analysis ===" << endl;
    
    MedicalImageProcessor med_processor(64, true);
    
    vector<string> case_types = {"normal", "tumor", "edema", "metastasis"};
    vector<MedicalImageProcessor::MRIImage> demo_cases;
    
    for (const string& case_type : case_types) {
        cout << "\n--- " << case_type << " Case Analysis ---" << endl;
        
        MedicalImageProcessor::MRIImage mri_case = med_processor.create_brain_mri(64, 64, case_type);
        mri_case.patient_id = "DEMO_" + case_type;
        demo_cases.push_back(mri_case);
        
        med_processor.visualize_mri_ascii(mri_case, true);
        
        Tensor mri_tensor = med_processor.mri_to_tensor(mri_case, true);
        
        medical_cnn.set_verbose(true);
        MedicalCNN::MedicalPrediction prediction = medical_cnn.predict(mri_tensor);
        
        cout << "\nClinical Assessment:" << endl;
        cout << "- Diagnosis: " << prediction.diagnosis << endl;
        cout << "- Confidence: " << fixed << setprecision(1) << (prediction.confidence * 100) << "%" << endl;
        
        Tensor heatmap = medical_cnn.get_activation_heatmap();
        med_processor.create_activation_heatmap(mri_case, heatmap, "Diagnostic Focus Areas");
    }
    
    cout << "\n=== Comprehensive Clinical Evaluation ===" << endl;
    MedicalEvaluationUtils::evaluate_clinical_performance(medical_cnn, clinical_data);
    
    cout << "\n=== Medical Interpretation Workflow Demo ===" << endl;
    MedicalEvaluationUtils::demonstrate_medical_workflow(medical_cnn, clinical_data);
    
    cout << "\n=== Mathematical Neural Network Library Summary ===" << endl;
    cout << "Comprehensive deep learning system implemented!" << endl;
    cout << string(60, '=') << endl;
    cout << "Core Components:" << endl;
    cout << "âœ“ Tensor operations with automatic differentiation" << endl;
    cout << "âœ“ Complete CNN architecture (3 conv + 2 FC layers)" << endl;
    cout << "âœ“ Image processing pipeline with synthetic data generation" << endl;
    cout << "âœ“ Medical CNN for brain tumor detection" << endl;
    cout << "âœ“ Clinical evaluation metrics (sensitivity, specificity)" << endl;
    cout << "âœ“ Activation heatmaps for medical interpretation" << endl;
    cout << "âœ“ Multiple activation functions (ReLU, Sigmoid, Tanh)" << endl;
    cout << "âœ“ Advanced optimizers (SGD, Adam, RMSProp)" << endl;
    cout << "âœ“ Loss functions with exact backpropagation" << endl;
    cout << "âœ“ Feature visualization and activation heatmaps" << endl;
    cout << "âœ“ Mathematical precision verified with gradient checking" << endl;
    cout << string(60, '=') << endl;
    cout << "Performance Metrics:" << endl;
    cout << "- Computer Vision CNN Parameters: " << (32*3*3*3 + 32 + 64*32*3*3 + 64 + 128*64*3*3 + 128 + 512*128 + 512 + 5*512 + 5) << endl;
    cout << "- Medical CNN Parameters: " << (8*1*3*3 + 8 + 16*8*3*3 + 16 + 128*4096 + 128 + 2*128 + 2) << endl;
    cout << "- Input Resolution: 32x32 RGB (general) | 64x64 Grayscale (medical)" << endl;
    cout << "- Applications: Pattern recognition + Medical tumor detection" << endl;
    cout << "- Gradient Computation: Verified accurate across all components" << endl;
    cout << "System ready for advanced deep learning and medical AI applications!" << endl;

    return 0;
}